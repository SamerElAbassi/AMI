{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "xgb_classifier = xgb.XGBClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('glove/glove.twitter.27B.200d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_embeddings(texts,embeddings):\n",
    "    means = []\n",
    "    dim = len(list(embeddings.values())[0])\n",
    "    for text in texts :\n",
    "        text = nltk.WordPunctTokenizer().tokenize(text)\n",
    "        means.append(np.mean([embeddings[w] if w in embeddings else np.zeros(dim) for w in text], axis=0))\n",
    "    return np.array(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATAFRAME_vect = pd.read_csv('AMI2020_TrainingSet/AMI2020_training_raw.tsv', sep='\\t')\n",
    "vectorizer = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "    strip_accents='unicode', analyzer='word', token_pattern=r'\\b[^\\d\\W]+\\b',\n",
    "    ngram_range=(2, 3),use_idf=True)\n",
    "DATAFRAME_test="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_emb=pd.read_csv('different processed dataframes/noun_chuncks,processed.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def run_cv(k_fold, data,label,text_data=None):\n",
    "    accuracy_scores = f1_scores = confusion_matrices = []\n",
    "    labels = np.array(label)\n",
    "    skf = StratifiedKFold(k_fold)\n",
    "    cv_splits = skf.split(data,labels)\n",
    "    min_inidices = ([],[])\n",
    "    min_acc = 100\n",
    "    media=0\n",
    "    for train, test in cv_splits:\n",
    "        traindata,y_train,= data[train],labels[train]\n",
    "        testdata,y_test=data[test],labels[test]\n",
    "        \n",
    "        model = LogisticRegression(penalty='l2', dual=True, tol=0.0001, max_iter=100000,\n",
    "                         C=3, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         solver = 'liblinear', warm_start=False,\n",
    "                         class_weight=None, random_state=None)\n",
    "      \n",
    "        '''\n",
    "        traintext,testtext=text_data[train],text_data[test]\n",
    "        train_emb,test_emb=get_mean_embeddings(traintext,embeddings_index),get_mean_embeddings(testtext,embeddings_index)\n",
    "        traindata=hstack((train_emb,traindata))\n",
    "        testdata=hstack((test_emb,testdata))\n",
    "        '''\n",
    "        model.fit(traindata,y_train)\n",
    "        result=model.predict(testdata)\n",
    "        score = accuracy_score(y_test, result)\n",
    "        accuracy_scores.append(score)\n",
    "        if score < min_acc:\n",
    "            min_acc = score\n",
    "            split_inidices = (train, test)\n",
    "        f1sc = f1_score(y_test, result, average='weighted')\n",
    "        media=media+f1sc\n",
    "        print('f1score:',f1sc)\n",
    "        f1_scores.append(f1sc)\n",
    "    print (f'min cv acc:{min_acc}\\nmedia:{media}')\n",
    "    print(np.mean(f1_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}