{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings,TransformerWordEmbeddings,SentenceTransformerDocumentEmbeddings,StackedEmbeddings,DocumentPoolEmbeddings\n",
    "from flair.data import Sentence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from scipy.sparse import hstack\n",
    "import spacy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used for testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_confusion(confusion_matrices):\n",
    "    avg = np.mean(confusion_matrices, axis=0)\n",
    "    total = sum(avg)\n",
    "    return avg*100/total\n",
    "def run_cv(classifier, k_fold, data, labels, runs=1):\n",
    "    accuracy_scores = []\n",
    "    f1scores = []\n",
    "    auc_scores = [0]\n",
    "    confusion_matrices = []\n",
    "    min_acc = 100\n",
    "    labels = np.array(labels)\n",
    "    skf = StratifiedKFold(k_fold)\n",
    "    for run in range(0, runs):\n",
    "        print(f'r{run}------------')\n",
    "        cv_splits = skf.split(data, labels)\n",
    "        for train, test in cv_splits:\n",
    "            traindata = data[train]\n",
    "            y_traindata = labels[train]\n",
    "            testdata = data[test]\n",
    "            y_testdata = labels[test]\n",
    "            model = clone(classifier)\n",
    "            model.fit(traindata, y_traindata)\n",
    "            result = model.predict(testdata)\n",
    "            score = accuracy_score(y_testdata, result)\n",
    "            accuracy_scores.append(score)\n",
    "            f1sc = f1_score(y_testdata, result, average='weighted')  \n",
    "            print(f1sc)\n",
    "            f1scores.append(f1sc)\n",
    "            #auc = roc_auc_score(y_testdata, result, average='weighted', multi_class='ovr')\n",
    "            #auc_scores.append(auc)\n",
    "            if f1sc < min_acc:\n",
    "                min_acc = f1sc \n",
    "                split_inidices = (train, test)\n",
    "            confusion_matrices.append(np.array(confusion_matrix(y_testdata, result)))\n",
    "    print (\"min cv F1: \", min_acc)   \n",
    "    return (accuracy_scores, f1scores, auc_scores, average_confusion(confusion_matrices), split_inidices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmbzembedding = TransformerWordEmbeddings('dbmdz/bert-base-italian-uncased')\n",
    "#flair_embedding_forward = FlairEmbeddings('it-forward')\n",
    "#flair_embedding_backward = FlairEmbeddings('it-backward')\n",
    "\n",
    "# document_embeddings = DocumentPoolEmbeddings([dmbzembedding,\n",
    "#                                               flair_embedding_backward,\n",
    "#                                               flair_embedding_forward\n",
    "#                                              ])\n",
    "document_embeddings =DocumentPoolEmbeddings([dmbzembedding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = 'different processed dataframes/raw.tsv' \n",
    "DATA_FRAME = pd.read_csv(CORPUS, '\\t',dtype=str)\n",
    "X=DATA_FRAME['text'].values\n",
    "Y=DATA_FRAME['misogynous'].values\n",
    "\n",
    "\n",
    "#Used the first time to get the embeddings. Embeddings.pkl contains embedded sentences using the stacked document embeddings (all 3)\n",
    "# data=[]\n",
    "# for i,text in enumerate(X):\n",
    "#     sentence=Sentence(text)\n",
    "#     try:\n",
    "#         document_embeddings.embed(sentence)\n",
    "#         data.append(sentence.embedding)\n",
    "#         print(i,'done')\n",
    "#     except:\n",
    "#         print(f'error at',i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding and tfidf both perform worse with any other data than the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CORPUS = 'different processed dataframes/noun_chuncks,processed.tsv' \n",
    "# DATA_FRAME = pd.read_csv(CORPUS, '\\t',dtype=str)\n",
    "# X=DATA_FRAME['clean'].values.astype(str)\n",
    "# Y=DATA_FRAME['misogynous'].values\n",
    "\n",
    "# #Used the first time to get the embeddings. Failed now but it once fully worked. Data is saved in embeddings.pkl\n",
    "# data=[]\n",
    "# for i,text in enumerate(X):\n",
    "#     sentence=Sentence(str(text))\n",
    "#     try:\n",
    "#         document_embeddings.embed(sentence)\n",
    "#         data.append(sentence.embedding)\n",
    "#         print(i,'done')\n",
    "#     except:\n",
    "#         print(f'error at',i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert tensors to np array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_array=np.array([tensor.cpu().detach().numpy() for tensor in data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For loading other pkl'ed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tensors bert.pkl','wb') as f:\n",
    "#     pickle.dump(emb_array, f)\n",
    "    \n",
    "# emb_array=np.array([tensor.cpu().detach().numpy() for tensor in data])\n",
    "# with open('only_bert_embeddings.pkl','wb') as f:\n",
    "#     pickle.dump(emb_array, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(penalty='l2', dual=True, tol=0.0001, max_iter=100000,\n",
    "                         C=3, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         solver = 'liblinear', warm_start=False,\n",
    "                         class_weight=None, random_state=None)\n",
    "svm=SVC()\n",
    "forest=RandomForestClassifier(n_estimators=1000,n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings only (nu mai rulez o data dar worse oricum, cu vreo 2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_array=[]\n",
    "with open('embeddings.pkl','rb') as f:\n",
    "    emb_array=pickle.load(f)\n",
    "    \n",
    "print(f'array of shape{emb_array.shape}\\nfirst element:{emb_array[0]}')\n",
    "\n",
    "run_cv(forest,10,emb_array,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 6722)\n",
      "r0------------\n",
      "0.9238025528069581\n",
      "0.9178437000052386\n",
      "0.915739551708546\n",
      "0.8755490909090908\n",
      "0.9398441206370721\n",
      "0.8933878444261625\n",
      "0.8080860215053762\n",
      "0.8261761732678617\n",
      "0.8858691023440595\n",
      "0.8391070626723972\n",
      "r1------------\n",
      "0.9238025528069581\n",
      "0.9178437000052386\n",
      "0.915739551708546\n",
      "0.8755490909090908\n",
      "0.9398441206370721\n",
      "0.8933878444261625\n",
      "0.8080860215053762\n",
      "0.8261761732678617\n",
      "0.8858691023440595\n",
      "0.8391070626723972\n",
      "r2------------\n",
      "0.9238025528069581\n",
      "0.9178437000052386\n",
      "0.915739551708546\n",
      "0.8755490909090908\n",
      "0.9398441206370721\n",
      "0.8933878444261625\n",
      "0.8080860215053762\n",
      "0.8261761732678617\n",
      "0.8858691023440595\n",
      "0.8391070626723972\n",
      "min cv F1:  0.8080860215053762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.924,\n",
       "  0.918,\n",
       "  0.916,\n",
       "  0.876,\n",
       "  0.94,\n",
       "  0.894,\n",
       "  0.808,\n",
       "  0.826,\n",
       "  0.886,\n",
       "  0.84,\n",
       "  0.924,\n",
       "  0.918,\n",
       "  0.916,\n",
       "  0.876,\n",
       "  0.94,\n",
       "  0.894,\n",
       "  0.808,\n",
       "  0.826,\n",
       "  0.886,\n",
       "  0.84,\n",
       "  0.924,\n",
       "  0.918,\n",
       "  0.916,\n",
       "  0.876,\n",
       "  0.94,\n",
       "  0.894,\n",
       "  0.808,\n",
       "  0.826,\n",
       "  0.886,\n",
       "  0.84],\n",
       " [0.9238025528069581,\n",
       "  0.9178437000052386,\n",
       "  0.915739551708546,\n",
       "  0.8755490909090908,\n",
       "  0.9398441206370721,\n",
       "  0.8933878444261625,\n",
       "  0.8080860215053762,\n",
       "  0.8261761732678617,\n",
       "  0.8858691023440595,\n",
       "  0.8391070626723972,\n",
       "  0.9238025528069581,\n",
       "  0.9178437000052386,\n",
       "  0.915739551708546,\n",
       "  0.8755490909090908,\n",
       "  0.9398441206370721,\n",
       "  0.8933878444261625,\n",
       "  0.8080860215053762,\n",
       "  0.8261761732678617,\n",
       "  0.8858691023440595,\n",
       "  0.8391070626723972,\n",
       "  0.9238025528069581,\n",
       "  0.9178437000052386,\n",
       "  0.915739551708546,\n",
       "  0.8755490909090908,\n",
       "  0.9398441206370721,\n",
       "  0.8933878444261625,\n",
       "  0.8080860215053762,\n",
       "  0.8261761732678617,\n",
       "  0.8858691023440595,\n",
       "  0.8391070626723972],\n",
       " [0],\n",
       " array([[89.77403294, 13.35286731],\n",
       "        [10.22596706, 86.64713269]]),\n",
       " (array([   0,    1,    2, ..., 4997, 4998, 4999]),\n",
       "  array([1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414,\n",
       "         1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425,\n",
       "         1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436,\n",
       "         1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447,\n",
       "         1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458,\n",
       "         1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469,\n",
       "         1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480,\n",
       "         1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491,\n",
       "         1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502,\n",
       "         1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513,\n",
       "         1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524,\n",
       "         1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535,\n",
       "         1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546,\n",
       "         1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557,\n",
       "         1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568,\n",
       "         1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579,\n",
       "         1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590,\n",
       "         1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601,\n",
       "         1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612,\n",
       "         1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623,\n",
       "         1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634,\n",
       "         1635, 1636, 1637, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431,\n",
       "         3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442,\n",
       "         3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453,\n",
       "         3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464,\n",
       "         3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475,\n",
       "         3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486,\n",
       "         3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497,\n",
       "         3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508,\n",
       "         3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519,\n",
       "         3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530,\n",
       "         3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541,\n",
       "         3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552,\n",
       "         3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563,\n",
       "         3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574,\n",
       "         3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585,\n",
       "         3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596,\n",
       "         3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607,\n",
       "         3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618,\n",
       "         3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629,\n",
       "         3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640,\n",
       "         3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651,\n",
       "         3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662,\n",
       "         3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673,\n",
       "         3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684,\n",
       "         3685, 3686, 3687, 3688, 3689])))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "                             strip_accents='unicode', analyzer='word', \n",
    "                             token_pattern=r'\\b[^\\d\\W]+\\b',\n",
    "                             ngram_range=(1, 2),use_idf=True)\n",
    "tfidf_X=vectorizer.fit_transform(X)\n",
    "print(tfidf_X.shape)\n",
    "\n",
    "run_cv(logistic,10,tfidf_X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arr_emb_tfidf=hstack((emb_array,tfidf_X)).toarray()\n",
    "#run_cv(logistic,10,arr_emb_tfidf,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with two models, one for the sparse tfidf features, and then using the predictions as a result for the random forest classifier (o aberatie deocamdata, am bagat predicitiile trainului la train features si dupa presupun ca forestu i-a acordat mare importanta so yeah e un tfidf mai prost, dar e pus aici as a memo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_2models(classifier_sparse,classifier_dense,sparse_features,dense_features, k_fold,data, labels, runs=1):\n",
    "    accuracy_scores = []\n",
    "    f1scores = []\n",
    "    auc_scores = [0]\n",
    "    confusion_matrices = []\n",
    "    min_acc = 100\n",
    "    labels = np.array(labels)\n",
    "    skf = StratifiedKFold(k_fold)\n",
    "    for run in range(0, runs):\n",
    "        print(f'---{run}---')\n",
    "        cv_splits = skf.split(data, labels)\n",
    "        for train, test in cv_splits:\n",
    "            \n",
    "            traindata_sparse,y_train = sparse_features[train],labels[train]\n",
    "            testdata_sparse,y_test = sparse_features[test],labels[test]\n",
    "            \n",
    "            traindata_dense,testdata_dense=dense_features[train],dense_features[test]\n",
    "            \n",
    "            #First model\n",
    "            model = clone(classifier_sparse)\n",
    "            model.fit(traindata_sparse, y_train)\n",
    "            result = model.predict(testdata_sparse)\n",
    "            result_train=model.predict(traindata_sparse)\n",
    "            \n",
    "            \n",
    "            #Adding features for the second model\n",
    "            result=result.reshape(500,1) ###Asta pentru 10 k-folduri\n",
    "            result_train=result_train.reshape(4500,1)\n",
    "            testdata_dense=np.hstack((testdata_dense,result))\n",
    "            traindata_dense=np.hstack((traindata_dense,result_train))\n",
    "\n",
    "            \n",
    "            model=clone(classifier_dense)\n",
    "            model.fit(traindata_dense,y_train)\n",
    "            result=model.predict(testdata_dense)\n",
    "            score = accuracy_score(y_test, result)\n",
    "            accuracy_scores.append(score)\n",
    "            f1sc = f1_score(y_test, result, average='weighted')  \n",
    "            print(f1sc)\n",
    "            f1scores.append(f1sc)\n",
    "            #auc = roc_auc_score(y_testdata, result, average='weighted', multi_class='ovr')\n",
    "            #auc_scores.append(auc)\n",
    "            if f1sc < min_acc:\n",
    "                min_acc = f1sc \n",
    "                split_inidices = (train, test)\n",
    "            confusion_matrices.append(np.array(confusion_matrix(y_test, result)))\n",
    "    print (\"min cv F1: \", min_acc)   \n",
    "    return (accuracy_scores, f1scores, auc_scores, average_confusion(confusion_matrices), split_inidices)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic pe sparse si random forest pe dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---0---\n",
      "0.9216487978931271\n",
      "0.9299479147813425\n",
      "0.9118511094281248\n",
      "0.8390217162133672\n",
      "0.9133822697022481\n",
      "0.8916072727272727\n",
      "0.7896024876040003\n",
      "0.7940271921087685\n",
      "0.8658954695469547\n",
      "0.8475131014158499\n",
      "min cv F1:  0.7896024876040003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.922, 0.93, 0.912, 0.84, 0.914, 0.892, 0.79, 0.794, 0.866, 0.848],\n",
       " [0.9216487978931271,\n",
       "  0.9299479147813425,\n",
       "  0.9118511094281248,\n",
       "  0.8390217162133672,\n",
       "  0.9133822697022481,\n",
       "  0.8916072727272727,\n",
       "  0.7896024876040003,\n",
       "  0.7940271921087685,\n",
       "  0.8658954695469547,\n",
       "  0.8475131014158499],\n",
       " [0],\n",
       " array([[88.68431147, 14.66778103],\n",
       "        [11.31568853, 85.33221897]]),\n",
       " (array([   0,    1,    2, ..., 4997, 4998, 4999]),\n",
       "  array([1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414,\n",
       "         1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425,\n",
       "         1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436,\n",
       "         1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447,\n",
       "         1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458,\n",
       "         1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469,\n",
       "         1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480,\n",
       "         1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491,\n",
       "         1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502,\n",
       "         1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513,\n",
       "         1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524,\n",
       "         1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535,\n",
       "         1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546,\n",
       "         1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557,\n",
       "         1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568,\n",
       "         1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579,\n",
       "         1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590,\n",
       "         1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601,\n",
       "         1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612,\n",
       "         1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623,\n",
       "         1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634,\n",
       "         1635, 1636, 1637, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431,\n",
       "         3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442,\n",
       "         3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453,\n",
       "         3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464,\n",
       "         3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475,\n",
       "         3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486,\n",
       "         3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497,\n",
       "         3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508,\n",
       "         3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519,\n",
       "         3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530,\n",
       "         3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541,\n",
       "         3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552,\n",
       "         3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563,\n",
       "         3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574,\n",
       "         3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585,\n",
       "         3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596,\n",
       "         3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607,\n",
       "         3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618,\n",
       "         3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629,\n",
       "         3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640,\n",
       "         3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651,\n",
       "         3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662,\n",
       "         3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673,\n",
       "         3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684,\n",
       "         3685, 3686, 3687, 3688, 3689])))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_cv_2models(logistic,forest,tfidf_X,emb_array,10,X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
