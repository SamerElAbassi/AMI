{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from scipy.sparse import coo_matrix, hstack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ugly but functional function\n",
    "def create_file(name,mis_labels,agr_labels):\n",
    "    indexes=[x for x in range(1,1001)]\n",
    "    data={'id':indexes,\n",
    "    'misogynous':mis_labels,\n",
    "    'aggressiveness':agr_labels}\n",
    "    dataframe=pd.DataFrame(data,index=indexes,columns=['id','misogynous','aggressiveness'])\n",
    "    dataframe.to_csv(name,sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run 1 constained tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_train = pd.read_csv('AMI2020_TrainingSet/AMI2020_training_raw.tsv', sep='\\t')\n",
    "DATAFRAME_test=pd.read_csv('test/AMI2020_test_raw.tsv',sep='\\t')\n",
    "DATAFRAME=pd.concat([DATAFRAME_train,DATAFRAME_test])\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "    strip_accents='unicode', analyzer='word', token_pattern=r'\\b[^\\d\\W]+\\b',\n",
    "    ngram_range=(1, 5),use_idf=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X=vectorizer.fit_transform(DATAFRAME['text'])\n",
    "X_train,X_test=X[:5000],X[5000:]\n",
    "model = LogisticRegression(penalty='l2', dual=True, tol=0.0001, max_iter=100000,\n",
    "                         C=3, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         solver = 'liblinear', warm_start=False,\n",
    "                         class_weight=None, random_state=None)\n",
    "\n",
    "model.fit(X_train,DATAFRAME_train['misogynous'])\n",
    "result_mis=model.predict(X_test)\n",
    "model.fit(X_train,DATAFRAME_train['aggressiveness'])\n",
    "result_agr=model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,(mis,agr) in enumerate(zip(result_mis,result_agr)):\n",
    "    if mis==0 and agr==1:\n",
    "        result_agr[i]=0 #Sunt 4 cazuri cand se intampla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_file('MDD.A.r.c.run1',result_mis,result_agr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconstrained Run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_embeddings(texts,embeddings):\n",
    "    means = []\n",
    "    dim = len(list(embeddings.values())[0])\n",
    "    for text in texts :\n",
    "        text = nltk.WordPunctTokenizer().tokenize(text)\n",
    "        means.append(np.mean([embeddings[w] if w in embeddings else np.zeros(dim) for w in text], axis=0))\n",
    "    return np.array(means)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('glove/glove.twitter.27B.200d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_train = pd.read_csv('AMI2020_TrainingSet/AMI2020_training_raw.tsv', sep='\\t')\n",
    "DATAFRAME_test=pd.read_csv('test/AMI2020_test_raw.tsv',sep='\\t')\n",
    "DATAFRAME=pd.concat([DATAFRAME_train,DATAFRAME_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=DATAFRAME['text'].values\n",
    "X_train,X_test=get_mean_embeddings(X[:5000],embeddings_index),get_mean_embeddings(X[5000:],embeddings_index)\n",
    "\n",
    "model = LogisticRegression(penalty='l2', dual=True, tol=0.0001, max_iter=100000,\n",
    "                         C=3, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         solver = 'liblinear', warm_start=False,\n",
    "                         class_weight=None, random_state=None)\n",
    "\n",
    "model.fit(X_train,DATAFRAME_train['misogynous'])\n",
    "result_mis=model.predict(X_test)\n",
    "model.fit(X_train,DATAFRAME_train['aggressiveness'])\n",
    "result_agr=model.predict(X_test)\n",
    "\n",
    "for i,(mis,agr) in enumerate(zip(result_mis,result_agr)):\n",
    "    if mis==0 and agr==1:\n",
    "        result_agr[i]=0 #Sunt 4 cazuri cand se intampla\n",
    "\n",
    "create_file('MDD.A.r.u.run2',result_mis,result_agr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconstrained run 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from noun_chunks import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    text = [w for w in text if '@' not in w]\n",
    "    text = [w for w in text if 'http' not in w]\n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Tokenize each word\n",
    "    text = nltk.WordPunctTokenizer().tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'astype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-47ff839ccf19>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mDATAFRAME_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mDATAFRAME_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDATAFRAME_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mDATAFRAME_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDATAFRAME_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'str'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'astype'"
     ]
    }
   ],
   "source": [
    "DATAFRAME_train = pd.read_csv('different processed dataframes/noun_chuncks,processed.tsv', sep='\\t')\n",
    "DATAFRAME_test=pd.read_csv('test/AMI2020_test_raw.tsv',sep='\\t')\n",
    "# n a mers pe jupyter\n",
    "# nlp = spacy.load('it_core_news_sm')\n",
    "\n",
    "import it_core_news_sm\n",
    "nlp=it_core_news_sm.load()\n",
    "\n",
    "text_list = []\n",
    "for i, text in enumerate(DATAFRAME_train['text']):\n",
    "    doc = nlp(text)\n",
    "    ncs = noun_chunks(doc)\n",
    "    chunks = [doc[nc[0]:nc[1]].text.lower() for nc in ncs]\n",
    "    text_chunks = str(\" \".join(chunks))\n",
    "    text_list.append(text_chunks)\n",
    "\n",
    "\n",
    "DATAFRAME_train['clean'] = text_list\n",
    "DATAFRAME_train['clean'] = list(map(clean_text, DATAFRAME_train['clean']))\n",
    "DATAFRAME_train['clean'] = [' '.join(text) for text in DATAFRAME_train['clean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_train['clean']=DATAFRAME_train['clean'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=DATAFRAME_train['clean'].values.astype('str')\n",
    "X_train,X_test=get_mean_embeddings(DATAFRAME_train['clean'],embeddings_index),get_mean_embeddings(DATAFRAME_test['text'],embeddings_index)\n",
    "model = LogisticRegression(penalty='l2', dual=True, tol=0.0001, max_iter=100000,\n",
    "                         C=3, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         solver = 'liblinear', warm_start=False,\n",
    "                         class_weight=None, random_state=None)\n",
    "\n",
    "\n",
    "model.fit(X_train,DATAFRAME_train['misogynous'])\n",
    "result_mis=model.predict(X_test)\n",
    "model.fit(X_train,DATAFRAME_train['aggressiveness'].values)\n",
    "result_agr=model.predict(X_test)\n",
    "\n",
    "\n",
    "for i,(mis,agr) in enumerate(zip(result_mis,result_agr)):\n",
    "    if mis==0 and agr==1:\n",
    "        result_agr[i]=0 \n",
    "        \n",
    "create_file('MDD.A.r.u.run3',result_mis,result_agr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK B RUN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFRAME_train_synt = pd.read_csv('AMI2020_TrainingSet/AMI2020_training_synt.tsv', sep='\\t')\n",
    "DATAFRAME_train_raw=pd.read_csv('AMI2020_TrainingSet/AMI2020_training_raw.tsv', sep='\\t')\n",
    "DATAFRAME_train=pd.concat([DATAFRAME_train_raw,DATAFRAME_train_synt],ignore_index=True)\n",
    "\n",
    "DATAFRAME_test_synt=pd.read_csv('test/AMI2020_test_synt.tsv',sep='\\t')\n",
    "DATAFRAME_test_raw=pd.read_csv('test/AMI2020_test_raw.tsv',sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_embeddings(texts,embeddings):\n",
    "    means = []\n",
    "    dim = len(list(embeddings.values())[0])\n",
    "    for text in texts :\n",
    "        text = nltk.WordPunctTokenizer().tokenize(text)\n",
    "        means.append(np.mean([embeddings[w] if w in embeddings else np.zeros(dim) for w in text], axis=0))\n",
    "    return np.array(means)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('glove/glove.twitter.27B.200d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,X_test_raw,X_test_synt=get_mean_embeddings(DATAFRAME_train['text'],embeddings_index),get_mean_embeddings(DATAFRAME_test_raw['text'],embeddings_index),get_mean_embeddings(DATAFRAME_test_synt['text'],embeddings_index)\n",
    "\n",
    "model = LogisticRegression(penalty='l2', dual=True, tol=0.0001, max_iter=100000,\n",
    "                         C=3, fit_intercept=True, intercept_scaling=1.0, \n",
    "                         solver = 'liblinear', warm_start=False,\n",
    "                         class_weight=None, random_state=None)\n",
    "\n",
    "#Raw test\n",
    "model.fit(X_train,DATAFRAME_train['misogynous'])\n",
    "result_mis=model.predict(X_test_raw)\n",
    "create_file_2('MDD.B.r.u.run2',result_mis)\n",
    "\n",
    "#Synt test\n",
    "model.fit(X_train,DATAFRAME_train['misogynous'])\n",
    "result_mis=model.predict(X_test_synt)\n",
    "create_file_2('MDD.B.s.u.run2',result_mis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_2(name,mis_labels):\n",
    "    indexes=[x for x in range(1,len(mis_labels)+1)]\n",
    "    data={'id':indexes,\n",
    "    'misogynous':mis_labels}\n",
    "    dataframe=pd.DataFrame(data,index=indexes,columns=['id','misogynous'])\n",
    "    dataframe.to_csv(name,sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
